{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "numerical-monthly",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## PHYS 105A:  Introduction to Scientific Computing\n",
    "\n",
    "# Minimization or Maximization\n",
    "\n",
    "Chi-kwan Chan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "historic-husband",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Problem Definition\n",
    "\n",
    "* We learned about root finding last time.  This week we will learn a related topic: minimization and maximization of functions.\n",
    "\n",
    "* The problem is similar, given a function $f(x)$, where $x$ may be a vector, we want to solve for the value of $x$ such that $f(x)$ is locally or globally a minimum or maximum.\n",
    "\n",
    "* This seemingly simple problem turns out to be EXTREMELY powerful!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polar-milan",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Applications\n",
    "\n",
    "* In theoretical physics, the *Action Principle* states that: the path taken by the system is the one for which the action is stationary (no change) to first order.\n",
    "\n",
    "  ![](https://upload.wikimedia.org/wikipedia/commons/1/1c/Least_action_principle.svg)\n",
    "  \n",
    "* In experimental physics, the *measured values* of your experiement are the solution to [maximize a likelyhood function](https://github.com/uarizona-2021spring-phys105a/phys105a/blob/main/06/dataproc.ipynb).\n",
    "  \n",
    "* In Very-Long-Baseline Interferometry, the [images you saw](https://eventhorizontelescope.org/blog/astronomers-image-magnetic-fields-edge-m87s-black-hole) are the maximum entropy images that fit the data.\n",
    "  \n",
    "* In most of the production ready [machine learning applications](https://playground.tensorflow.org/), the machine learning models are also mimimal solutions of some loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "running-circus",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Optimization\n",
    "\n",
    "* In general, minimizing or maximizing a funciton falls into a field of mathematics called optimization.\n",
    "\n",
    "* Because so many problems can be casted into a optimization, it is a very important field with lots of applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swiss-winter",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Methods\n",
    "\n",
    "* For one-dimension minimization problems, there are two major classes of methods:\n",
    "\n",
    "  * Methods that require only evaluation of the function\n",
    "  \n",
    "  * Methods that require also evaluations of the derivative of the function.\n",
    "  \n",
    "* The first class of method is similar to the bisection method we learned last week for root finding.  The second class of methods is similar to the Newton Raphson method that we skipped last time.\n",
    "\n",
    "* For multivariable problems, there is one more class of methods where you can compute the derivatives using finite difference.\n",
    "\n",
    "* Unlike root finding, methods that require derivatives are easier to understand, so we will learn them in this lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "described-franklin",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Graphical Search for Extrema\n",
    "\n",
    "* Just like looking for roots, it is always a good idea to plot a function.\n",
    "\n",
    "* We already know how to plot functions in python\n",
    "\n",
    "* In fact, let's use the same polynomial we used last time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adolescent-nursing",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def h(x):\n",
    "    return x**5 - 16*x**3 + 32*x + 4\n",
    "\n",
    "x = np.linspace(-4, 4, 8001)\n",
    "plt.plot(x, h(x))\n",
    "plt.axhline(y=0, color='k', lw=0.5)\n",
    "\n",
    "# The extrema are at:\n",
    "# Global minimum: -4\n",
    "# Global maximum: +4\n",
    "# Local minima: ≈ -1, -3\n",
    "# Local maxima: ≈ 3, 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atmospheric-parcel",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gradient Descent Method\n",
    "\n",
    "* There are many methods for solving the minimum once we have the derivative.\n",
    "\n",
    "* They have different speed and complexity.  One of the simplest but easiest to implment is gradient descent.\n",
    "\n",
    "* The idea is very simple:\n",
    "\n",
    "  * Evaluate the function's derivative at a given point.\n",
    "  \n",
    "  * Step toward the \"downhill\" direction.\n",
    "  \n",
    "  * Repeat until the derivative is small enough that you are near a minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacterial-flooring",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def minimize(f, f_x, x, alpha, acc=1e-3, nmax=1000):\n",
    "    \"\"\"Minimize function f, with derivative f_x, around x.\n",
    "    Use alpha as the step.\n",
    "    \"\"\"\n",
    "    # The _ in Python means \"I won't use this variable\"\n",
    "    for _ in range(nmax):\n",
    "        y   = f(x)\n",
    "        y_x = f_x(x)\n",
    "        if abs(y_x) <= acc:\n",
    "            return x         \n",
    "        x -= alpha * y_x\n",
    "            \n",
    "    raise Exception(\"Too many iterations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stone-myrtle",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Let's test it\n",
    "\n",
    "#def h(x):\n",
    "#    return x**5 - 16*x**3 + 32*x + 4\n",
    "\n",
    "def h_x(x):\n",
    "    return 5 * x**4 - 48 * x**2 + 32\n",
    "    \n",
    "m0 = minimize(h, h_x, 0, 1e-3)\n",
    "m1 = minimize(h, h_x, 2, 1e-3)\n",
    "\n",
    "x = np.linspace(-4, 4, 8001)\n",
    "plt.plot(x, h(x))\n",
    "plt.axhline(y=0,  color='k', lw=0.5)\n",
    "plt.axvline(x=m0, color='r', lw=0.5)\n",
    "plt.axvline(x=m1, color='r', lw=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "killing-discharge",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Finding maximum is also easy\n",
    "\n",
    "def maximize(f, f_x, x, alpha, **kwargs):\n",
    "    def nf(x):\n",
    "        return -f(x)\n",
    "    def nf_x(x):\n",
    "        return -h_x(x)\n",
    "    return minimize(nf, nf_x, x, alpha, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recorded-zoning",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Let's also test it\n",
    "\n",
    "M0 = maximize(h, h_x, -2, 1e-3)\n",
    "M1 = maximize(h, h_x,  0, 1e-3)\n",
    "\n",
    "x = np.linspace(-4, 4, 8001)\n",
    "plt.plot(x, h(x))\n",
    "plt.axhline(y=0,  color='k', lw=0.5)\n",
    "plt.axvline(x=M0, color='r', lw=0.5)\n",
    "plt.axvline(x=M1, color='r', lw=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southeast-regulation",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How Good is our Optimizer?\n",
    "\n",
    "* Within less than an hour, we implemented a gradient descent method.  This is AWESOME because we can now solve some problem that even the greatest mathematicians cannot solve!\n",
    "\n",
    "* However, this is too good to be true.  While gradient descent is a generic enough method, there are many traps that we may fall into that break our optimizer.\n",
    "\n",
    "* Instead of learning a more complicated optimization algorithm, let's try to break our gradient descent method and then fix it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hidden-secondary",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# We use 1e-3 as the step size; what if we change it to a larger step?\n",
    "\n",
    "m1 = minimize(h, h_x, 2, 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "internal-least",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# What's going on?\n",
    "# To understand why the algorithm breaks, let's modify our\n",
    "# gradient descent method output more information.\n",
    "\n",
    "def minimize(f, f_x, x, alpha, acc=1e-3, nmax=1000):\n",
    "    # Returning the guesses for the minimyn\n",
    "    l = np.array([x])\n",
    "    for _ in range(nmax):\n",
    "        y   = f(x)\n",
    "        y_x = f_x(x)\n",
    "        if abs(y_x) <= acc:\n",
    "            return l\n",
    "        x -= alpha * y_x\n",
    "        l = np.append(l, x)\n",
    "            \n",
    "    raise Exception(\"Too many iterations\", l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "connected-reader",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    l1 = minimize(h, h_x, 2, 1e-2)\n",
    "except Exception as e:\n",
    "    print('Failed')\n",
    "    l1 = e.args[1]\n",
    "    \n",
    "print(l1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attempted-solution",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "n = 4\n",
    "    \n",
    "x = np.linspace(-4, 4, 8001)\n",
    "plt.plot(x, h(x))\n",
    "plt.plot(l1[:n], h(l1[:n]), 'o-')\n",
    "plt.axhline(y=0,  color='k', lw=0.5)\n",
    "plt.xlim(2, 3.5)\n",
    "plt.ylim(-100, -50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fallen-malawi",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Take Away\n",
    "\n",
    "* For simple gradient descent, the step size is important.\n",
    "\n",
    "* If a step is too big, you may end up jump around the extrema without reaching the accuracy requirements.\n",
    "\n",
    "* Adjusting the step size to reach a fast and accurate convergence is all *hyperparameter tuning*.\n",
    "\n",
    "* There are many algorithms to automatically adjust the step size.\n",
    "\n",
    "* However, for this course, we may simply adjust the step size by visually inspecting the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "national-armor",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Moving on to Multidimensional Problems\n",
    "\n",
    "* Because gradient descent is so simple, we can trivially generalize it to multiple variables.\n",
    "\n",
    "* This will enables to solve much more intereting questions including curve fittings, or even physics problems that uses the action principle.\n",
    "\n",
    "* The basic idea is that instead of moving left or right in one dimension, the vector of derivatives gives you the gradient of the function.\n",
    "\n",
    "* For simplicity, we will move one direction at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "owned-internship",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Let's implement a two-dimension minimizer\n",
    "\n",
    "def minimize(f, f_x, f_y, x, y, alpha, acc=1e-3, nmax=1000):\n",
    "    l = np.array([x, y])\n",
    "    for i in range(nmax):\n",
    "        z   = f(x, y)\n",
    "        z_x = f_x(x, y)\n",
    "        z_y = f_y(x, y)\n",
    "        if z_x*z_x + z_y*z_y <= acc * acc:\n",
    "            return l\n",
    "        if i % 2 == 0:\n",
    "            x -= alpha * z_x\n",
    "        else:\n",
    "            y -= alpha * z_y\n",
    "        l = np.vstack((l, [x, y]))\n",
    "            \n",
    "    raise Exception(\"Too many iterations\", l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "approximate-yellow",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Test our implementation\n",
    "\n",
    "def g(x, y):\n",
    "    return (x - 2)**2 + (y - 1)**2\n",
    "\n",
    "def g_x(x, y):\n",
    "    return 2 * (x - 2)\n",
    "\n",
    "def g_y(x, y):\n",
    "    return 2 * (y - 1)\n",
    "\n",
    "try:\n",
    "    l1 = minimize(g, g_x, g_y, 0, 0, 0.1)\n",
    "except Exception as e:\n",
    "    print('Failed')\n",
    "    l1 = e.args[1]\n",
    "    \n",
    "print(l1[-1,:])\n",
    "plt.plot(l1[:,0], l1[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confidential-wesley",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "* Optimization is an important field in mathematics with many applications!\n",
    "\n",
    "* Numerical optimizers can help us find extrema of functions complicated.\n",
    "\n",
    "* The gradient descent method is extremely simple but powerful!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intensive-prediction",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
